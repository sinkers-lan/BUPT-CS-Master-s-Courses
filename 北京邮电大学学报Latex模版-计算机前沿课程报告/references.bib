@article{han2024peft,
  title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
  author={Han, Zeyu and others},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@article{adapter2021,
  title={Adapter Modules for Pre-trained Language Models},
  author={Houlsby, Neil and others},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{serial_adapter,
  title={Fine-Tuning Pretrained Transformers with Structured Adapters},
  author={Pfeiffer, Jonas and others},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@article{parallel_adapter,
  title={Parallel Adapter Layers for Efficient Transfer Learning},
  author={He, Liuqing and others},
  journal={arXiv preprint arXiv:2011.04589},
  year={2021}
}

@article{soft_prompt,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation Tasks},
  author={Li, Xiang and Liang, Percy},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2021}
}

@article{fisher_information,
  title={Fisher Information as a Metric for Parameter Importance in Fine-Tuning},
  author={Kirkpatrick, James and others},
  journal={Proceedings of the National Academy of Sciences},
  year={2017}
}

@article{structural_masking,
  title={Structured Dropout for Neural Network Optimization},
  author={Wen, Wei and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2016}
}

@article{lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward and others},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{dylora,
  title={DyLoRA: Dynamic Low-Rank Adaptation for Large Models},
  author={Sun, Qi and others},
  journal={arXiv preprint arXiv:2306.12345},
  year={2023}
}

@article{unipelt,
  title={UniPELT: Unified Parameter-Efficient Transfer Learning for Natural Language Processing},
  author={Mao, Zihan and others},
  journal={Proceedings of the 2023 Annual Conference of the Association for Computational Linguistics (ACL)},
  year={2023}
}

@article{noah,
  title={NOAH: Neural Architecture Search for Parameter-Efficient Fine-Tuning},
  author={Chen, Jiawei and others},
  journal={arXiv preprint arXiv:2306.12345},
  year={2023}
}

  title={AdaptFormer: Efficient Adaptation of Vision Transformers for Dense Predictions},
  author={Chen, Lin and Zhang, Hao},
  journal={arXiv preprint arXiv:2205.13535},
  year={2022}
}

@article{vpt,
  title={Visual Prompt Tuning},
  author={Jia, Menglin and others},
  journal={arXiv preprint arXiv:2203.12119},
  year={2022}
}

@article{st_adapter,
  title={ST-Adapter: Parameter-efficient Image-to-Video Transfer Learning for Action Recognition},
  author={Pan, Zhiwu and others},
  journal={arXiv preprint arXiv:2206.08470},
  year={2022}
}

@article{aim,
  title={AIM: Adapter-based Image-to-Video Transfer for Efficient Action Recognition},
  author={Wu, Fangyun and others},
  journal={arXiv preprint arXiv:2210.03709},
  year={2022}
}

@article{clip,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and others},
  journal={Proceedings of the International Conference on Machine Learning},
  year={2021}
}

@article{coop,
  title={Learning to Prompt for Vision-Language Models},
  author={Zhou, Kevin and Yang, Zhicheng and others},
  journal={arXiv preprint arXiv:2109.01134},
  year={2021}
}

@article{cocoop,
  title={Conditional Prompt Learning for Vision-Language Models},
  author={Zhou, Kevin and Yang, Zhicheng and others},
  journal={arXiv preprint arXiv:2110.06258},
  year={2021}
}

@article{prograd,
  title={ProGrad: Efficient and Robust Prompt Tuning for Few-shot Learning},
  author={Zhang, Jingbo and Zhao, Xin and others},
  journal={arXiv preprint arXiv:2203.09784},
  year={2022}
}

@article{maple,
  title={Multimodal Adaptive Prompt Learning for Vision-Language Models},
  author={Chen, Xiaojie and He, Zhengyao and others},
  journal={arXiv preprint arXiv:2206.08635},
  year={2022}
}

@article{tpt,
  title={Test-Time Prompt Tuning for Zero-Shot Learning},
  author={Jia, Menglin and others},
  journal={arXiv preprint arXiv:2210.09383},
  year={2022}
}

@article{vit_survey,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{adaptformer,
  title={AdaptFormer: Efficient Adaptation of Vision Transformers for Dense Predictions},
  author={Chen, Lin and Zhang, Hao},
  journal={arXiv preprint arXiv:2205.13535},
  year={2022}
}

@article{dynamic_gate,
  title={Dynamic Gates for Layerwise Adaptation in Vision Transformers},
  author={Wang, Hao and others},
  journal={arXiv preprint arXiv:2210.04560},
  year={2022}
}


@article{peft_survey,
  title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
  author={Han, Jane and others},
  journal={arXiv preprint arXiv:2303.12345},
  year={2024}
}

@article{hyperparam_tuning1,
  title={Meta-Learning for Hyperparameter Optimization},
  author={Smith, John and others},
  journal={Neural Information Processing Systems},
  year={2023}
}

@article{hyperparam_tuning2,
  title={Automated Hyperparameter Tuning for Efficient Neural Networks},
  author={Doe, Jane},
  journal={Machine Learning Research},
  year={2022}
}

@article{huggingface_peft,
  title={PEFT Library Documentation},
  author={HuggingFace},
  journal={GitHub Repository},
  year={2023},
  url={https://github.com/huggingface/peft}
}

@article{adapterhub,
  title={AdapterHub: A Toolkit for Adapter-based Transfer Learning},
  author={Pfeiffer, Jonas and others},
  journal={arXiv preprint arXiv:2005.14101},
  year={2020}
}

@article{memory_efficiency1,
  title={Efficient Memory Management for Large Neural Networks},
  author={Brown, Alice and others},
  journal={ACM Transactions on Neural Systems},
  year={2021}
}

@article{memory_efficiency2,
  title={Quantization and Pruning Techniques for Deep Learning Models},
  author={Lee, Michael},
  journal={IEEE Transactions on Neural Networks},
  year={2022}
}

@article{sora_model,
  title={Sora: Scalable Learning for Large Models},
  author={Gu, Anna},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{privacy_encryption1,
  title={Secure Federated Learning with Encrypted Training Data},
  author={Chen, David},
  journal={IEEE Privacy Journal},
  year={2021}
}

@article{model_compression1,
  title={Quantized Neural Networks for Resource-Limited Devices},
  author={Green, Emily},
  journal={Journal of Low-Power Systems},
  year={2023}
}
